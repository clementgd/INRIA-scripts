{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8299625-9261-415a-b76a-f3b64759cde6",
   "metadata": {},
   "source": [
    "# Scheduling benchmarks analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369384b4-fd06-4968-9403-864730261c06",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88befc-dc73-401c-b951-787c8302589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21abd6e-21af-4c25-a3e2-9608cb35b85f",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461acef-279d-4cd7-8b92-2af4519dadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_CMD_CACHE_FILENAME = \"trace_cmd_runtimes\"\n",
    "RESULTS_DIR_PATH = \"../results\"\n",
    "ABSOLUTE_RESULTS_DIR_PATH = \"/home/cgachod/analysis/results\"\n",
    "\n",
    "def get_result_dir_path(result_dir_name):\n",
    "    return os.path.join(RESULTS_DIR_PATH, result_dir_name)\n",
    "\n",
    "def get_absolute_result_dir_path(result_dir_name) :\n",
    "    return os.path.join(ABSOLUTE_RESULTS_DIR_PATH, result_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e405e89-f2f9-41a7-87b3-ec4bc67833fc",
   "metadata": {},
   "source": [
    "#### Visualization params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752714b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_enabled_color = \"tab:orange\"\n",
    "nb_disabled_color = \"tab:blue\"\n",
    "\n",
    "figure_height = 10 # 10\n",
    "min_figure_width = 40\n",
    "figure_width_coeff = 0.3\n",
    "\n",
    "line_width = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f884db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_runs_plot():\n",
    "    # plt.figure(figsize=(fig_width, 5))\n",
    "    # plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"Nth run\")\n",
    "    plt.ylabel(\"Run time (seconds)\")\n",
    "    plt.grid(axis=\"y\", which=\"both\")\n",
    "    plt.grid(axis=\"x\", which=\"major\")\n",
    "    \n",
    "    # ax = plt.gca()\n",
    "    # ax.ticklabel_format(axis=\"x\", useOffset=1.0)\n",
    "\n",
    "def plot_runs(values: List[float], label = None, color = None, init_plot = True) :\n",
    "    if init_plot:\n",
    "        init_runs_plot()\n",
    "    plt.plot(range(len(values)), values, label=label, color=color, linewidth=line_width)\n",
    "    \n",
    "    current_xticks_count = len(plt.xticks()[0])\n",
    "    if len(values) > current_xticks_count :\n",
    "        plt.xticks(range(len(values)), rotation=45)\n",
    "        plt.xlim(-1, len(values))\n",
    "        \n",
    "    # candidate_ylim_max = max(values) * 1.2\n",
    "    # if candidate_ylim_max > plt.ylim()[1] :\n",
    "    #     plt.ylim(0, candidate_ylim_max)\n",
    "        \n",
    "    fig = plt.gcf()\n",
    "    candidate_width = max(int(len(values) * figure_width_coeff), min_figure_width)\n",
    "    curr_width = fig.get_size_inches()[0]\n",
    "    if candidate_width > curr_width :\n",
    "        fig.set_size_inches(candidate_width, figure_height)\n",
    "    \n",
    "    if label :\n",
    "        plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64ea1-b811-42f2-87bf-0846e47c4579",
   "metadata": {},
   "source": [
    "# Perf stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d522009-6e95-4786-987b-4e1d1379864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perf_stats(run_dir_path) -> List[float]:\n",
    "    hyperfine_run_files = [file for file in os.listdir(run_dir_path) if file.endswith(\".json\")]\n",
    "    if len(hyperfine_run_files) == 0 :\n",
    "        return []\n",
    "    if len(hyperfine_run_files) > 1 :\n",
    "        print(\"WARNING : More than 1 hyperfine run files found in\", run_dir_path)\n",
    "    \n",
    "    with open(os.path.join(run_dir_path, hyperfine_run_files[0])) as f:\n",
    "        data = json.load(f)\n",
    "    return data['results'][0]['times']\n",
    "\n",
    "\n",
    "# Idea here is to create a df with all the counter values\n",
    "# so n cols = (min, max, avg) * counters\n",
    "# rows are runs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerfStatRun :\n",
    "    def __init__(self, file_path) -> None :\n",
    "        self.file_path = file_path\n",
    "        self.event_counters = self.parse_file(file_path)\n",
    "        self.event_counters = self.compute_additional_counters(self.event_counters)\n",
    "        self.available_events = set(self.event_counters.keys()) # set([cv[\"event\"] for cv in self.event_counters])\n",
    "        \n",
    "    def compute_additional_counters(self, event_counters: Dict[str, float]) :\n",
    "        if \"branches\" in event_counters and \"branch-misses\" in event_counters :\n",
    "            event_counters[\"branch-misses-ratio\"] = event_counters[\"branch-misses\"] / event_counters[\"branches\"]\n",
    "        \n",
    "        if \"l2_rqsts.all_demand_references\" in event_counters and \"l2_rqsts.all_demand_miss\" in event_counters :\n",
    "            event_counters[\"l2_rqsts.all_demand_miss-ratio\"] = event_counters[\"l2_rqsts.all_demand_miss\"] / event_counters[\"l2_rqsts.all_demand_references\"]\n",
    "        \n",
    "        if \"l2_rqsts.all_demand_data_rd\" in event_counters and \"l2_rqsts.demand_data_rd_miss\" in event_counters :\n",
    "            event_counters[\"l2_rqsts.demand_data_rd_miss-ratio\"] = event_counters[\"l2_rqsts.demand_data_rd_miss\"] / event_counters[\"l2_rqsts.all_demand_data_rd\"]\n",
    "        \n",
    "        if \"LLC-loads\" in event_counters and \"LLC-load-misses\" in event_counters :\n",
    "            event_counters[\"LLC-load-misses-ratio\"] = event_counters[\"LLC-load-misses\"] / event_counters[\"LLC-loads\"]\n",
    "        \n",
    "        if \"LLC-stores\" in event_counters and \"LLC-store-misses\" in event_counters :\n",
    "            event_counters[\"LLC-store-misses-ratio\"] = event_counters[\"LLC-store-misses\"] / event_counters[\"LLC-stores\"]\n",
    "        \n",
    "        if \"cache-references\" in event_counters and \"cache-misses\" in event_counters :\n",
    "            event_counters[\"cache-misses-ratio\"] = event_counters[\"cache-misses\"] / event_counters[\"cache-references\"]\n",
    "        return event_counters\n",
    "        \n",
    "    def parse_file(self, file_path) -> Dict[str, float]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        nas_time_match = re.search(r'Time in seconds\\s+=\\s+(\\d*.\\d*)', text)\n",
    "        if nas_time_match :\n",
    "            nas_time = float(nas_time_match.group(1))\n",
    "\n",
    "        json_begin_pos = text.find('{')\n",
    "        json_end_pos = text.rfind('}')\n",
    "        json_text = text[json_begin_pos:json_end_pos + 1]\n",
    "        json_lines = json_text.split('\\n')\n",
    "        first_line_obj = json.loads(line)\n",
    "        if \"cpu\" in first_line_obj :\n",
    "            return self.parse_file_per_cpu(file_path)\n",
    "        \n",
    "        event_counters = {\"nas_runtime\": nas_time}\n",
    "        for line in json_lines :\n",
    "            json_object = json.loads(line)\n",
    "            event_counters[json_object[\"event\"].strip()] = float(json_object[\"counter-value\"])\n",
    "\n",
    "        return event_counters\n",
    "    \n",
    "    def parse_file_per_cpu(self, file_path) -> Dict[str, float]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        nas_time_match = re.search(r'Time in seconds\\s+=\\s+(\\d*.\\d*)', text)\n",
    "        if nas_time_match :\n",
    "            nas_time = float(nas_time_match.group(1))\n",
    "\n",
    "        json_begin_pos = text.find('{')\n",
    "        json_end_pos = text.rfind('}')\n",
    "        json_text = text[json_begin_pos:json_end_pos + 1]\n",
    "        json_lines = json_text.split('\\n')\n",
    "        event_counters = {\"nas_runtime\": nas_time}\n",
    "        for line in json_lines :\n",
    "            json_object = json.loads(line)\n",
    "            event_counters[json_object[\"event\"].strip()] = float(json_object[\"counter-value\"])\n",
    "\n",
    "        return event_counters\n",
    "    \n",
    "    def get_event_value(self, event: str) :\n",
    "        # return float(self.event_counters[event][\"counter-value\"])\n",
    "        return self.event_counters[event]\n",
    "    \n",
    "    \n",
    "class PerfStatBenchmark :\n",
    "    def __init__(self, benchmark_dir_path) -> None:\n",
    "        self.available_events, self.results_per_directory = self.read_benchmark_dir(benchmark_dir_path)\n",
    "        \n",
    "    def sort_by_event(self, event_name: str) :\n",
    "        for dir_name, results in self.results_per_directory.items() :\n",
    "            results.sort(key = lambda x : x.get_event_value(event_name))\n",
    "        \n",
    "    def get_values_for_event(self, event_name: str) -> Dict[str, List[float]] :\n",
    "        res = {}\n",
    "        for dir_name, results in self.results_per_directory.items() :\n",
    "            res[dir_name] = [r.get_event_value(event_name) for r in results]\n",
    "        return res\n",
    "    \n",
    "    def print_available_events(self) :\n",
    "        print(\"Available events\")\n",
    "        for i, e in enumerate(self.available_events) :\n",
    "            print(f\"{i}. {e}\")\n",
    "            \n",
    "    def parse_runs(self, runs_dir_path: str) -> List[PerfStatRun] :\n",
    "        files = [file for file in os.listdir(runs_dir_path) if file.endswith(\".txt\")]\n",
    "        return [PerfStatRun(os.path.join(runs_dir_path, file)) for file in files]\n",
    "    \n",
    "    def read_benchmark_dir(self, benchmark_dir_path: str) -> Tuple[List[str], Dict[str, List[PerfStatRun]]] :\n",
    "        available_events = None\n",
    "        runs_results = {}\n",
    "        \n",
    "        contents = os.listdir(benchmark_dir_path)\n",
    "        for c in contents :\n",
    "            dir_path = os.path.join(benchmark_dir_path, c)\n",
    "            if not os.path.isdir(dir_path) :\n",
    "                continue\n",
    "            \n",
    "            perf_results = self.parse_runs(dir_path)\n",
    "            runs_results[c] = perf_results\n",
    "            available_events_list = [pr.available_events for pr in perf_results]\n",
    "            available_events_intersection = set.intersection(*available_events_list)\n",
    "            if available_events is None :\n",
    "                available_events = available_events_intersection\n",
    "            else :\n",
    "                available_events.intersection(available_events_intersection)\n",
    "            \n",
    "        available_events = list(available_events)\n",
    "        return available_events, runs_results\n",
    "        \n",
    "        \n",
    "\n",
    "def parse_perf_stat_result_file(file_path: str) -> PerfStatRun :\n",
    "    return PerfStatRun(file_path)\n",
    "\n",
    "def get_perf_stats_run(run_dir_path: str) :\n",
    "    files = [file for file in os.listdir(run_dir_path) if file.endswith(\".txt\")]\n",
    "    return [parse_perf_stat_result_file(os.path.join(run_dir_path, file)) for file in files]\n",
    "    \n",
    "\n",
    "# We want 2 things :\n",
    "# 1. A graph with a line per directory\n",
    "# 2. A graph combining all folders with time in x axis and value in y\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Idea is one graph per value, or simply change the value for the graph\n",
    "    \n",
    "def read_perf_stat_benchmark_dir(dir_name: str) :\n",
    "    return PerfStatBenchmark(get_result_dir_path(dir_name))\n",
    "\n",
    "\n",
    "def plot_perf_stat_benchmark_for_event(perf_stat_benchmark: PerfStatBenchmark, event_name: int, sort_by_event: str = \"\") :\n",
    "    if len(sort_by_event) != -1 :\n",
    "        perf_stat_benchmark.sort_by_event(sort_by_event)\n",
    "    \n",
    "    values_for_event = perf_stat_benchmark.get_values_for_event(event_name)\n",
    "    print(\"Values for event :\", event_name)\n",
    "    init_runs_plot()\n",
    "    for dir_name, values in values_for_event.items() :\n",
    "        plot_runs(values=values, label=dir_name, init_plot=False)\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def plot_perf_stat_benchmark_for_events(perf_stat_benchmark: PerfStatBenchmark, events: List[str] = [], sort_by_event: str = \"\") :\n",
    "    if len(sort_by_event) == 0 :\n",
    "        sort_by_event = \"duration_time\"\n",
    "    if len(events) == 0 :\n",
    "        events = perf_stat_benchmark.available_events\n",
    "    \n",
    "    for e in events :\n",
    "        plot_perf_stat_benchmark_for_event(perf_stat_benchmark, e, sort_by_event)\n",
    "\n",
    "\n",
    "events = [\n",
    "    \"nas_runtime\",\n",
    "    \"duration_time\",\n",
    "    \"system_time\",\n",
    "    \"user_time\",\n",
    "    \"instructions\",\n",
    "    \"cycles\",\n",
    "    \"migrations\",\n",
    "    \"context-switches\",\n",
    "    \"cache-misses\",\n",
    "    \"cache-misses-ratio\",\n",
    "    \"LLC-loads\",\n",
    "    \"LLC-load-misses\",\n",
    "    \"LLC-load-misses-ratio\",\n",
    "    \"LLC-stores\",\n",
    "    \"LLC-store-misses\",\n",
    "    \"LLC-store-misses-ratio\",\n",
    "    \"cycle_activity.stalls_l3_miss\",\n",
    "    \"l2_rqsts.all_demand_miss\",\n",
    "    \"l2_rqsts.all_demand_miss-ratio\",\n",
    "    \"l2_rqsts.demand_data_rd_miss\",\n",
    "    \"l2_rqsts.demand_data_rd_miss-ratio\",\n",
    "    \"mem_load_l3_miss_retired.local_dram\",\n",
    "    \"mem_load_l3_miss_retired.remote_dram\",\n",
    "    \"offcore_response.all_data_rd.l3_miss_local_dram.snoop_miss_or_no_fwd\",\n",
    "    \"offcore_response.all_data_rd.l3_miss_remote_dram.snoop_miss_or_no_fwd\",\n",
    "    \"branch-misses\",\n",
    "    \"branch-misses-ratio\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f67e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 things :\n",
    "# 1. Plot the boxes per event and use the fact that 1 setup is slower than the others\n",
    "# 2. Mix all the data from a directory and try to derive a correlation to runtime for each event\n",
    "\n",
    "\n",
    "def parse_per_cpu_perf_file(file_path) -> Tuple[pd.DataFrame, Dict]:\n",
    "    # min:... max:...\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    nas_time_match = re.search(r'Time in seconds\\s+=\\s+(\\d*.\\d*)', text)\n",
    "    if nas_time_match :\n",
    "        nas_time = float(nas_time_match.group(1))\n",
    "        \n",
    "        \n",
    "    # Make it a dataframe ?\n",
    "    # columns would be event\n",
    "    # lines cpu\n",
    "\n",
    "    json_begin_pos = text.find('{')\n",
    "    json_end_pos = text.rfind('}')\n",
    "    json_text = text[json_begin_pos:json_end_pos + 1]\n",
    "    json_lines = json_text.split('\\n')\n",
    "    \n",
    "    events_df = pd.DataFrame()\n",
    "    \n",
    "    meta_values = {\"nas_runtime\": nas_time}\n",
    "    \n",
    "    event_counters: Dict[str, list] = defaultdict(list)\n",
    "    for line in json_lines :\n",
    "        json_object = json.loads(line)\n",
    "        event_str = json_object[\"event\"].strip()\n",
    "        if event_str in [\"duration_time\", \"user_time\", \"system_time\"] :\n",
    "            meta_values[event_str] = float(json_object[\"counter-value\"])\n",
    "        else :\n",
    "            event_counters[event_str].append((int(json_object[\"cpu\"]), float(json_object[\"counter-value\"])))\n",
    "        \n",
    "    events_df = pd.DataFrame({event: [e[1] for e in sorted(values)] for event, values in event_counters.items()})\n",
    "    events_df[\"LLC-all-misses\"] = events_df[\"LLC-load-misses\"] + events_df[\"LLC-store-misses\"]\n",
    "    events_df[\"LLC-load-misses-ratio\"] = events_df[\"LLC-load-misses\"] / events_df[\"LLC-loads\"]\n",
    "    events_df[\"LLC-store-misses-ratio\"] = events_df[\"LLC-store-misses\"] / events_df[\"LLC-stores\"]\n",
    "    \n",
    "    events_df[\"mem_load_l3_miss_retired.all\"] = events_df[\"mem_load_l3_miss_retired.remote_dram\"] + events_df[\"mem_load_l3_miss_retired.local_dram\"]\n",
    "    events_df[\"mem_load_l3_miss_retired-over-LLC-all\"] = events_df[\"mem_load_l3_miss_retired.all\"] / events_df[\"LLC-all-misses\"]\n",
    "    events_df[\"mem_load_l3_miss_retired.remote_over_local_dram\"] = events_df[\"mem_load_l3_miss_retired.remote_dram\"] / events_df[\"mem_load_l3_miss_retired.local_dram\"]\n",
    "    events_df[\"mem_load_l3_miss_retired.remote_over_total\"] = events_df[\"mem_load_l3_miss_retired.remote_dram\"] / events_df[\"mem_load_l3_miss_retired.all\"]\n",
    "    events_df[\"mem_load_l3_miss_retired.local_over_total\"] = events_df[\"mem_load_l3_miss_retired.local_dram\"] / events_df[\"mem_load_l3_miss_retired.all\"]\n",
    "    \n",
    "    return events_df, meta_values\n",
    "    \n",
    "    # result_dict = {}\n",
    "    # for event, values in event_counters.items() :\n",
    "    #     if event in [\"nas_runtime\", \"duration_time\", \"user_time\", \"system_time\"] :\n",
    "    #         result_dict[f\"once:{event}\"] = min(values)\n",
    "    #     else :\n",
    "    #         result_dict[f\"min:{event}\"] = min(values)\n",
    "    #         result_dict[f\"max:{event}\"] = max(values)\n",
    "    #         result_dict[f\"sum:{event}\"] = sum(values)\n",
    "            \n",
    "    # # Computing new events\n",
    "    \n",
    "    # # ratio remote l3 miss / local l3 miss\n",
    "    # # ratio l3 miss / l3 access\n",
    "    \n",
    "    # return result_dict\n",
    "\n",
    "\n",
    "def parse_dir(dir_path) :\n",
    "    df = pd.DataFrame()\n",
    "    file_paths = [os.path.join(dir_path, file) for file in os.listdir(dir_path) if file.endswith(\".txt\")]\n",
    "    for file_path in file_paths :\n",
    "        events_df, result_dict = parse_per_cpu_perf_file(file_path)\n",
    "        for series_name, series in events_df.items() :\n",
    "            result_dict[f\"min:{series_name}\"] = series.min()\n",
    "            result_dict[f\"max:{series_name}\"] = series.max()\n",
    "            result_dict[f\"sum:{series_name}\"] = series.sum()\n",
    "        file_df = pd.DataFrame(result_dict, index=[0])\n",
    "        df = pd.concat([df, file_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def combine_benchmark_dir(benchmark_dir_name) :\n",
    "    df = pd.DataFrame()\n",
    "    benchmark_dir_path = get_result_dir_path(\"_perf/\" + benchmark_dir_name)\n",
    "    contents = os.listdir(benchmark_dir_path)\n",
    "    for c in contents :\n",
    "        dir_path = os.path.join(benchmark_dir_path, c)\n",
    "        if not os.path.isdir(dir_path) :\n",
    "            continue\n",
    "        \n",
    "        df = pd.concat([df, parse_dir(dir_path)], ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df, meta_values = parse_per_cpu_perf_file(get_result_dir_path(\"_perf/cg.C.x__dahu-21__v6.8.0-rc3__performance__2024-04-10-sample/nb-disabled-sockorder/cg.C.x__dahu-21__v6.8.0-rc3__performance__2024-04-10__nb-disabled-sockorder__1.txt\"))\n",
    "\n",
    "for col in events_df.columns :\n",
    "    print(col, sum(events_df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = combine_benchmark_dir(\"cg.C.x__dahu-21__v6.8.0-rc3__performance__2024-04-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "correlations = perf_df.corr()['nas_runtime']\n",
    "# print(perf_df.corr()['once:nas_runtime'].sort_values())\n",
    "\n",
    "\n",
    "\n",
    "# new_df = correlations.to_frame(name = \"corr\")\n",
    "# new_df[\"abs_corr\"] = new_df[\"corr\"].map(lambda x : abs(x))\n",
    "# new_df.sort_values(by = \"abs_corr\", ascending=False)\n",
    "\n",
    "correlations.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Print the box plots : one graph per event, on each graph 3 functions for all the setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e93019",
   "metadata": {},
   "source": [
    "### cg.C on Dahu 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = read_perf_stat_benchmark_dir(\"_perf/cg.C.x__dahu-11__v6.8.0-rc3__performance__2024-03-17\")\n",
    "# bench.print_available_events()\n",
    "\n",
    "# test_file_result_path = \"cg.C.x__dahu-11__v6.8.0-rc3__performance__2024-03-17/nb-disabled-none/cg.C.x__dahu-11__v6.8.0-rc3__performance__2024-03-17__nb-disabled-none__99.txt\"\n",
    "# result = parse_perf_stat_result_file(get_result_dir_path(test_file_result_path))\n",
    "# # print(result.event_counters)\n",
    "\n",
    "plot_perf_stat_benchmark_for_events(bench, events=events, sort_by_event=\"nas_runtime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241fe3e-d5f0-4b3d-adb9-ff67b3ab1921",
   "metadata": {},
   "source": [
    "TODOs :\n",
    "- Try and focus on the worst core instead of showing an average across all cores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "import math as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_CMD_CACHE_FILENAME = \"trace_cmd_runtimes\"\n",
    "RESULTS_DIR_PATH = \"../results\"\n",
    "ABSOLUTE_RESULTS_DIR_PATH = \"/home/cgachod/analysis/results\"\n",
    "\n",
    "WORKING_DIR = \"/root/tests\"\n",
    "PERF_SCRIPT_RESULTS_FILEPATH = f\"{WORKING_DIR}/.perf_mem_results.log\"\n",
    "\n",
    "NODE_1_PHYS_ADDR_START = 0x1840000000\n",
    "\n",
    "DAHU_NODE_0_CPUID = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62]\n",
    "DAHU_NODE_1_CPUID = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63]\n",
    "dahu_cpu_nodes = [1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)]\n",
    "\n",
    "def in_working_dir(path: str) :\n",
    "    return os.path.join(WORKING_DIR, path)\n",
    "\n",
    "def get_result_dir_path(result_dir_name):\n",
    "    return os.path.join(RESULTS_DIR_PATH, result_dir_name)\n",
    "\n",
    "def get_absolute_result_dir_path(result_dir_name) :\n",
    "    return os.path.join(ABSOLUTE_RESULTS_DIR_PATH, result_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_enabled_color = \"tab:orange\"\n",
    "nb_disabled_color = \"tab:blue\"\n",
    "\n",
    "figure_height = 10 # 10\n",
    "min_figure_width = 40\n",
    "figure_width_coeff = 0.3\n",
    "\n",
    "line_width = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to represent where each core accesses memory\n",
    "- Sort cores by node\n",
    "- For each core, have a timeline or maybe just a percentage of time where it accesses which node\n",
    "- Maybe we can have 2 histograms, one up which is local node and 1 down which is remote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So for that we only need cpu and memory address and time\n",
    "# We assume we have the perf data file\n",
    "\n",
    "# 1: pid, 2: tid, 3: cpuid, 4: time, 5: period, 6: event , 7: virtual address\n",
    "# basic_info_regex_str = r\"^ *[\\w\\/\\-\\.\\: ]+ +(\\d+) \\[(\\d+)\\] (\\d+\\.\\d+): +([\\w\\/\\-\\.]+).*?: +([0-9a-f]+)\"\n",
    "basic_info_regex_str = r\"^ *[\\w\\/\\-\\.\\:]+ +(\\d+)\\/(\\d+) +\\[(\\d+)\\] +(\\d+\\.\\d+): +(\\d+) +([\\w\\/\\-\\.]+).*?: +([0-9a-f]+)\"\n",
    "\n",
    "\n",
    "# 8: memory access type, 9: TLB access type\n",
    "data_src_regex_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+\"\n",
    "phys_addr_regex_str = r\"([0-9a-f]+)\"\n",
    "# data_src_regex = re.compile()\n",
    "\n",
    "basic_info_regex_lat_str = r\"^ *\\S+ +(\\d+) +(\\d+) +(\\d+\\.\\d+): +(\\d+) +(\\S+): +([0-9a-f]+)\"\n",
    "data_src_regex_lat_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+(\\d+) +\\d+ +([0-9a-f]+).+ ([0-9a-f]+)\"\n",
    "\n",
    "line_regex = re.compile(basic_info_regex_str + r\"\\s+\" + data_src_regex_str + phys_addr_regex_str)\n",
    "line_lat_regex = re.compile(basic_info_regex_lat_str + r\" +\" + data_src_regex_lat_str)\n",
    "\n",
    "def generate_perf_mem_log(file_path: str) -> dict :\n",
    "    command_str = f\"perf script -i {file_path} -c cg.C.x -F 'comm,pid,tid,cpu,time,period,event,addr,data_src,phys_addr' --time 10%-20% > {PERF_SCRIPT_RESULTS_FILEPATH}\"\n",
    "    print(command_str)\n",
    "    result = subprocess.run(\n",
    "        command_str,\n",
    "        shell=True,\n",
    "        stdout = subprocess.PIPE,\n",
    "        universal_newlines = True\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# perf script -L -F 'comm,pid,tid,cpu,time,addr,event,ip,phys_addr,data_src,period' --reltime --time 1.0,2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resgex = line_regex.match(\"          cg.C.x 10674/10674 [000]  2789.844649:      26661         cpu/mem-stores/P: ffffe9b88d9bfcd0      1e05080144 |OP STORE|LVL L1 or N/A hit|SNP N/A|TLB N/A|LCK N/A|BLK  N/A                                0\")\n",
    "print(resgex.group(6)[4:14] == \"mem-stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_perf_mem_log(in_working_dir(\"perf-mem-15-alone.data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_events_df(filepath: str) -> pd.DataFrame :\n",
    "    timestamp = []\n",
    "    cpuid = []\n",
    "    event = []\n",
    "    virtual_addr = []\n",
    "    physical_addr = []\n",
    "    cache_result = []\n",
    "    period = []\n",
    "\n",
    "    # filename = \"assets/perf_mem_sample.log\"\n",
    "    # filename = \"assets/.perf_mem_results.log\"\n",
    "    with open(filepath) as f :\n",
    "        for line in f :\n",
    "            matched = line_regex.match(line)\n",
    "            if matched :\n",
    "                event.append(matched[6])\n",
    "                timestamp.append(float(matched[4]))\n",
    "                cpuid.append(int(matched[3]))\n",
    "                cache_result.append(matched[8])\n",
    "                virtual_addr.append(int(matched[7], base=16))\n",
    "                physical_addr.append(int(matched[10], base=16))\n",
    "                period.append(int(matched[5]))\n",
    "                pass\n",
    "            else :\n",
    "                print(\"Not matched line : \", line)\n",
    "                \n",
    "    dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "    \n",
    "    events_df = pd.DataFrame({\n",
    "        \"time\": timestamp, \n",
    "        \"cpuid\": cpuid,  \n",
    "        \"event\": event,\n",
    "        \"virt\": virtual_addr, \n",
    "        \"phys\": physical_addr, \n",
    "        \"cache_result\": cache_result, \n",
    "        \"period\": period\n",
    "    })\n",
    "    events_df['time'] = events_df['time'] - events_df['time'].min()\n",
    "    # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "    events_df['cpu_node'] = events_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "    events_df['memory_node'] = (events_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "    return events_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_events_with_latency(filepath: str) -> pd.DataFrame :\n",
    "    basic_info_regex_lat_str = r\"^ *\\S+ +(\\d+) +(\\d+) +(\\d+\\.\\d+): +(\\d+) +(\\S+): +([0-9a-f]+)\"\n",
    "    data_src_regex_lat_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+(\\d+) +\\d+ +([0-9a-f]+).+ ([0-9a-f]+)\"\n",
    "    line_lat_regex = re.compile(basic_info_regex_lat_str + r\" +\" + data_src_regex_lat_str)\n",
    "    \n",
    "    timestamp = []\n",
    "    cpuid = []\n",
    "    event = []\n",
    "    virtual_addr = []\n",
    "    physical_addr = []\n",
    "    cache_result = []\n",
    "    period = []\n",
    "\n",
    "    # filename = \"assets/perf_mem_sample.log\"\n",
    "    # filename = \"assets/.perf_mem_results.log\"\n",
    "    with open(filepath) as f :\n",
    "        for line in f :\n",
    "            matched = line_lat_regex.match(line)\n",
    "            if matched :\n",
    "                event.append(matched[6])\n",
    "                timestamp.append(float(matched[4]))\n",
    "                cpuid.append(int(matched[3]))\n",
    "                cache_result.append(matched[8])\n",
    "                virtual_addr.append(int(matched[7], base=16))\n",
    "                physical_addr.append(int(matched[10], base=16))\n",
    "                period.append(int(matched[5]))\n",
    "                pass\n",
    "            else :\n",
    "                print(\"Not matched line : \", line)\n",
    "                \n",
    "    dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "    \n",
    "    events_df = pd.DataFrame({\n",
    "        \"time\": timestamp, \n",
    "        \"cpuid\": cpuid,  \n",
    "        \"event\": event,\n",
    "        \"virt\": virtual_addr, \n",
    "        \"phys\": physical_addr, \n",
    "        \"cache_result\": cache_result, \n",
    "        \"period\": period\n",
    "    })\n",
    "    events_df['time'] = events_df['time'] - events_df['time'].min()\n",
    "    # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "    events_df['cpu_node'] = events_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "    events_df['memory_node'] = (events_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "    return events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerfResultsReader:\n",
    "    def __init__(self, log_file_path = in_working_dir(\".perf.log\")) -> None:\n",
    "        self.log_file_path = log_file_path\n",
    "        \n",
    "    def __extract_perf_data_file_with_latency(self, data_file_path: str, executable: Optional[str]) -> bool:\n",
    "        executable_filter = f\"-c {executable}\" if executable is not None else \"\"\n",
    "        command_str = f\"perf script -i {data_file_path} -L {executable_filter} > {self.log_file_path}\"\n",
    "        print(command_str)\n",
    "        result = subprocess.run(\n",
    "            command_str,\n",
    "            shell=True,\n",
    "            stdout = subprocess.PIPE,\n",
    "            universal_newlines = True\n",
    "        )\n",
    "        return True\n",
    "    \n",
    "    def __parse_events_with_latency(self, filepath: str) -> pd.DataFrame :\n",
    "        # 1: pid, 2: cpuid, 3: timestamp, 4: period, 5: event, 6: virt_addr \n",
    "        basic_info_regex_lat_str = r\"^ *\\S+ +(\\d+) +(\\d+) +(\\d+\\.\\d+): +(\\d+) +(\\S+): +([0-9a-f]+)\"\n",
    "        \n",
    "        # 1: cache_result, 2: tlb_result, 3: latency, 4: phys_adress\n",
    "        data_src_regex_lat_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+(\\d+) +\\d+ +[0-9a-f]+.+ ([0-9a-f]+)\"\n",
    "        line_lat_regex = re.compile(basic_info_regex_lat_str + r\" +\" + data_src_regex_lat_str)\n",
    "        \n",
    "        # pid = []\n",
    "        cpuid = []\n",
    "        timestamp = []\n",
    "        period = []\n",
    "        event = []\n",
    "        virtual_addr = []\n",
    "        cache_result = []\n",
    "        latency = []\n",
    "        physical_addr = []\n",
    "\n",
    "        with open(filepath) as f :\n",
    "            for line in f :\n",
    "                matched = line_lat_regex.match(line)\n",
    "                if matched :\n",
    "                    # pid.append(int(matched[1]))\n",
    "                    cpuid.append(int(matched[2]))\n",
    "                    timestamp.append(float(matched[3]))\n",
    "                    period.append(int(matched[4]))\n",
    "                    event.append(matched[5])\n",
    "                    virtual_addr.append(int(matched[6], base=16))\n",
    "                    cache_result.append(matched[7])\n",
    "                    latency.append(int(matched[9]))\n",
    "                    physical_addr.append(int(matched[10], base=16))\n",
    "                    pass\n",
    "                else :\n",
    "                    print(\"Not matched line : \", line)\n",
    "                    \n",
    "        dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "        \n",
    "        events_df = pd.DataFrame({\n",
    "            \"cpuid\": cpuid,  \n",
    "            \"time\": timestamp, \n",
    "            \"period\": period,\n",
    "            \"event\": event,\n",
    "            \"virt\": virtual_addr,\n",
    "            \"phys\": physical_addr, \n",
    "            \"latency\": latency,\n",
    "            \"cache_result\": cache_result,\n",
    "        })\n",
    "        events_df['time'] = events_df['time'] - events_df['time'].min()\n",
    "        # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "        events_df['cpu_node'] = events_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "        events_df['memory_node'] = (events_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "        return events_df\n",
    "    \n",
    "    \n",
    "    def read_perf_data_with_latency(self, data_file_path: str, executable: Optional[str]) -> pd.DataFrame:\n",
    "        self.__extract_perf_data_file_with_latency(data_file_path, executable)\n",
    "        df = self.__parse_events_with_latency(self.log_file_path)\n",
    "        os.remove(self.log_file_path)\n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr = PerfResultsReader()\n",
    "# prr.extract_perf_data_file_with_latency(data_file_path=in_working_dir(\"perf.data\"), executable=\"cg.C.x\")\n",
    "# prr._PerfResultsReader__parse_events_with_latency(in_working_dir(\".perf.sample.log\"))\n",
    "events_df = prr.read_perf_data_with_latency(in_working_dir(\"perf.data\"), \"cg.C.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_memory_accesses_dfs(filepath: str) -> Tuple[pd.DataFrame, pd.DataFrame] :\n",
    "    # loads, stores\n",
    "    is_store = []\n",
    "    timestamp = []\n",
    "    cpuid = []\n",
    "    virtual_addr = []\n",
    "    physical_addr = []\n",
    "    cache_result = []\n",
    "    period = []\n",
    "\n",
    "    # filename = \"assets/perf_mem_sample.log\"\n",
    "    # filename = \"assets/.perf_mem_results.log\"\n",
    "    with open(filepath) as f :\n",
    "        for line in f :\n",
    "            matched = line_regex.match(line)\n",
    "            if matched :\n",
    "                is_store.append(int(matched[6][4:14] == \"mem-stores\"))\n",
    "                timestamp.append(float(matched[4]))\n",
    "                cpuid.append(int(matched[3]))\n",
    "                cache_result.append(matched[8])\n",
    "                virtual_addr.append(int(matched[7], base=16))\n",
    "                physical_addr.append(int(matched[10], base=16))\n",
    "                period.append(int(matched[5]))\n",
    "                pass\n",
    "            else :\n",
    "                print(\"Not matched line : \", line)\n",
    "                \n",
    "    dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "    \n",
    "    accesses_df = pd.DataFrame({\n",
    "        \"time\": timestamp, \n",
    "        \"cpuid\": cpuid,  \n",
    "        \"virt\": virtual_addr, \n",
    "        \"phys\": physical_addr, \n",
    "        \"cache_result\": cache_result, \n",
    "        \"period\": period,\n",
    "        \"is_store\": is_store\n",
    "    })\n",
    "    accesses_df['time'] = accesses_df['time'] - accesses_df['time'].min()\n",
    "    # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "    accesses_df['cpu_node'] = accesses_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "    accesses_df['memory_node'] = (accesses_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "    accesses_df = accesses_df.loc[accesses_df['phys'] != 0]\n",
    "    \n",
    "    loads_df = accesses_df.loc[accesses_df[\"is_store\"] == 0]\n",
    "    loads_df['time_offset'] = loads_df['time'].diff()\n",
    "    \n",
    "    stores_df = accesses_df.loc[accesses_df[\"is_store\"] == 1]\n",
    "    stores_df['time_offset'] = stores_df['time'].diff()\n",
    "    \n",
    "    return loads_df.iloc[1:], stores_df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_perf_results_with_latency(data_file_path: str, executable = \"cg.C.x\") -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    command_str = f\"perf script -i {data_file_path} -L -c {executable} --time 10%-20% > {PERF_SCRIPT_RESULTS_FILEPATH}\"\n",
    "    print(command_str)\n",
    "    result = subprocess.run(\n",
    "        command_str,\n",
    "        shell=True,\n",
    "        stdout = subprocess.PIPE,\n",
    "        universal_newlines = True\n",
    "    )\n",
    "    \n",
    "    events_df = build_events_df(in_working_dir(\"perf.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 664 894 lines\n",
    "loads_df, stores_df = build_memory_accesses_dfs(in_working_dir(\"perf.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = build_events_df(in_working_dir(\"perf.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores_df.loc[stores_df['cache_result'] == \"LVL L1 or N/A hit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.cm.Spectral_r([0,1])\n",
    "\n",
    "# for colorname in plt.colormaps() :\n",
    "#     display(plt.cm.get_cmap(colorname))\n",
    "# colormaps[\"turbo\"]([100, 128, 196, 70])\n",
    "# plt.cm.get_cmap(\"OrRd\")\n",
    "\n",
    "# colormaps[\"OrRd\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_results_stats(input_df: pd.DataFrame) :\n",
    "    period_mean = []\n",
    "    time_offset_mean = []\n",
    "    cache_results_counts = input_df['cache_result'].value_counts()\n",
    "    for key, value in cache_results_counts.items() :\n",
    "        period_mean.append(input_df.loc[input_df['cache_result'] == key]['period'].mean())\n",
    "        time_offset_mean.append(input_df.loc[input_df['cache_result'] == key]['time_offset'].mean())\n",
    "    result_df = cache_results_counts.to_frame()\n",
    "    result_df['period_mean'] = period_mean\n",
    "    result_df['time_offset_mean'] = time_offset_mean\n",
    "    return result_df.sort_values('period_mean', ascending=False)\n",
    "\n",
    "\n",
    "def plot_cache_results(input_df: pd.DataFrame) :\n",
    "    period_per_cr = {}\n",
    "    time_offset_per_cr = {}\n",
    "    cache_results_counts = input_df['cache_result'].value_counts()\n",
    "    for key, value in cache_results_counts.items() :\n",
    "        period_per_cr[key] = input_df.loc[input_df['cache_result'] == key]['period']\n",
    "        time_offset_per_cr[key] = input_df.loc[input_df['cache_result'] == key]['time_offset']\n",
    "        \n",
    "    # Period values\n",
    "    plt.boxplot(period_per_cr.values(), labels=period_per_cr.keys())\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Time offsets values\n",
    "    plt.boxplot(time_offset_per_cr.values(), labels=time_offset_per_cr.keys(), showfliers=False, showmeans=True)\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    # plt.gcf().set_size_inches(12, 50)\n",
    "    plt.show()\n",
    "    \n",
    "    ratio = {key: period_per_cr[key] / time_offset_per_cr[key] for key in period_per_cr.keys()}\n",
    "    plt.boxplot(ratio.values(), labels=period_per_cr.keys(), showfliers=False, showmeans=True)\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    # plt.gcf().set_size_inches(12, 50)\n",
    "    plt.show()\n",
    "    \n",
    "plot_cache_results(loads_df)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loads cache results stats\")\n",
    "print(get_cache_results_stats(loads_df))\n",
    "\n",
    "print(\"\\n\\nStores cache results stats\")\n",
    "print(get_cache_results_stats(stores_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scatter_accesses_for_cpu(accesses_df: pd.DataFrame, cpuid = -1, min_phys = None max_phys = None) :\n",
    "#     cache_results_df = get_cache_results_stats(accesses_df)\n",
    "#     color_map = colormaps[\"turbo\"]\n",
    "#     cache_results_colors = {cache_res_val: color_map(25 * i) for i, cache_res_val in enumerate(period_df.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cmap_id in plt.colormaps():\n",
    "#     print(cmap_id)\n",
    "#     display(colormaps[cmap_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# remote_memory_df['cache_result'].value_counts().items() # I want to have value counts and average period for each\n",
    "period_df = get_cache_results_stats(loads_df)\n",
    "period_df = period_df.sort_values('period_mean', ascending=False)\n",
    "\n",
    "print(period_df)\n",
    "\n",
    "color_map = colormaps[\"turbo\"]\n",
    "cache_results_colors = {cache_res_val: color_map(25 * i) for i, cache_res_val in enumerate(period_df.index)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_bounds(values: np.array, eps: int, min_group_size: int):\n",
    "    sorted_values = np.sort(values)\n",
    "    \n",
    "    clusters = []\n",
    "    curr_lower_idx = 0\n",
    "    curr_lower_val = sorted_values[0]\n",
    "    # current_group = [sorted_values[0]]\n",
    "\n",
    "    for i, val in enumerate(sorted_values):\n",
    "        group_size = i - curr_lower_idx - 1\n",
    "        if val - curr_lower_val > eps and group_size >= min_group_size :\n",
    "            clusters.append((curr_lower_val, sorted_values[i - 1], group_size))\n",
    "            curr_lower_idx = i\n",
    "            curr_lower_val = val\n",
    "            \n",
    "    return clusters\n",
    "\n",
    "def filter_in_bounds(df: pd.DataFrame, property: str, bounds: tuple) -> pd.DataFrame :\n",
    "    return df.loc[(df[property] > bounds[0]) & (df[property] < bounds[1])]\n",
    "\n",
    "\n",
    "# def get_virtual_addrs_clusters(data: dict) -> Tuple[list, list] :\n",
    "#     return get_clusters_1D(np.concatenate([np.array(cpu['virtual_addrs']) for cpu in data]), 1e7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loads_df.index, loads_df['phys'].sort_values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan :\n",
    "- Have a graph of the exact phys remote addresses, maybe we can see smth there\n",
    "- Otherwise scatter all the addresses but y axis only remote vs local node, and color according to type of cache hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_addr_subset_df = memory_df.loc[(memory_df['phys'] > bounds[0]) & (memory_df['phys'] < bounds[1])]\n",
    "phys_addr_subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuid = 1\n",
    "# subset_df = memory_df.loc[(memory_df['cache_result'] in [\"LVL L1 or L1 hit\", LVL Remote RAM hit])]\n",
    "subset_df = phys_addr_subset_df.loc[memory_df['cpuid'] == cpuid]\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(subset_df[\"time\"], subset_df[\"phys\"], s=6, alpha=0.5, c=colors[n])\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to print all of the accesses for a given CPU, with node 1 or 2 in Y and cache result as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = colormaps[\"turbo\"]\n",
    "load_cache_results_values = [\"LVL LFB/MAB or LFB/MAB hit\", \"LVL L1 or L1 hit\", \"LVL L2 or L2 hit\", \"LVL L3 or L3 hit\", \"LVL L3 miss\", \"LVL Local RAM or RAM hit\", \"LVL Remote Any cache hit\", \"LVL Remote RAM hit\"]\n",
    "print([color_map((255 / len(load_cache_results_values)) * i) for i, cache_res_val in enumerate(load_cache_results_values)])\n",
    "load_cache_results_colors = {cache_res_val: color_map(int((255 / len(load_cache_results_values)) * i)) for i, cache_res_val in enumerate(load_cache_results_values)}\n",
    "\n",
    "\n",
    "\n",
    "# tab10\n",
    "load_cache_results_colors = {cache_res_val: colormaps[\"tab10\"](i) for i, cache_res_val in enumerate(load_cache_results_values)}\n",
    "# color_map(100.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node_and_cache_result(df: pd.DataFrame, cache_results_colors: dict, cpuid: int, min_t: float = None, max_t: float = None) :\n",
    "    cpu_df = df.loc[df['cpuid'] == cpuid]\n",
    "    cpu_df['is_remote'] = np.int64(cpu_df['cpu_node'] != cpu_df['memory_node'])\n",
    "    \n",
    "    if min_t is not None:\n",
    "        cpu_df = cpu_df.loc[cpu_df['time'] >= min_t]\n",
    "        \n",
    "    if max_t is not None:\n",
    "        cpu_df = cpu_df.loc[cpu_df['time'] <= max_t]\n",
    "    \n",
    "    for result, color in cache_results_colors.items():\n",
    "        # print(\"Cache result :\", result, \"color : \")\n",
    "        subset = cpu_df.loc[cpu_df['cache_result'] == result]\n",
    "        # print(subset)\n",
    "        plt.scatter(subset['time'], subset['is_remote'], label=result, color=color, s=6, alpha=0.5)\n",
    "        # break\n",
    "\n",
    "    plt.gcf().set_size_inches(120, 10)\n",
    "    plt.grid(axis=\"y\", which=\"both\")\n",
    "    plt.grid(axis=\"x\", which=\"major\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phys')\n",
    "    plt.ylim(-3, 4)\n",
    "    plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "    plt.legend(title='Cache Result')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def print_address_and_cache_result(df: pd.DataFrame, cache_results_colors: dict, cpuid: int, min_t: float = None, max_t: float = None):\n",
    "    cpu_df = df.loc[df['cpuid'] == cpuid]\n",
    "    \n",
    "    if min_t is not None:\n",
    "        cpu_df = cpu_df.loc[cpu_df['time'] >= min_t]\n",
    "        \n",
    "    if max_t is not None:\n",
    "        cpu_df = cpu_df.loc[cpu_df['time'] <= max_t]\n",
    "        \n",
    "    subset_node0 = cpu_df.loc[cpu_df['memory_node'] == 0]\n",
    "    subset_node1 = cpu_df.loc[cpu_df['memory_node'] == 1]\n",
    "    \n",
    "    def plot_subset(node_df: pd.DataFrame) :\n",
    "        for result, color in cache_results_colors.items():\n",
    "            subset = node_df.loc[node_df['cache_result'] == result]\n",
    "            plt.scatter(subset['time'], subset['phys'], label=result, color=color, s=6, alpha=0.5)\n",
    "\n",
    "        plt.gcf().set_size_inches(120, 10)\n",
    "        plt.grid(axis=\"y\", which=\"both\")\n",
    "        plt.grid(axis=\"x\", which=\"major\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Phys')\n",
    "        plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "        plt.legend(title='Cache Result')\n",
    "        plt.show()\n",
    "        \n",
    "    plot_subset(subset_node0)\n",
    "    plot_subset(subset_node1)\n",
    "    \n",
    "    # Graph on top : node 0\n",
    "    # Graph on bottom : node 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuid = 15\n",
    "cpu_loads_df = loads_df.loc[loads_df['cpuid'] == cpuid]\n",
    "cpu_loads_clusters = get_clusters_1D(cpu_loads_df['phys'], 5e8, 100)\n",
    "cpu_loads_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds = load_clusters[0][3]\n",
    "# reduced_loads_df = loads_df.loc[(loads_df['phys'] > bounds[0]) & (loads_df['phys'] < bounds[1])]\n",
    "\n",
    "bounds = cpu_loads_clusters[0][0]\n",
    "reduced_loads_df = cpu_loads_df.loc[(cpu_loads_df['phys'] > bounds[0]) & (cpu_loads_df['phys'] < bounds[1])]\n",
    "# print_node_and_cache_result(reduced_loads_df, load_cache_results_colors, 15, 1.85, 2.0)\n",
    "print_address_and_cache_result(reduced_loads_df, load_cache_results_colors, 15)\n",
    "# print_address_and_cache_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_loads_df = loads_df.loc[loads_df['cpu_node'] != loads_df['memory_node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_loads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_loads_df['cpuid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_memory_df\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# colors = \n",
    "# colors = {result: plt.cm.tab20(i) for i, result in enumerate(remote_memory_df['cache_result'].unique())}\n",
    "cache_results_colors\n",
    "for result, color in cache_results_colors.items():\n",
    "    # print(\"Cache result :\", result, \"color : \")\n",
    "    subset = remote_loads_df.loc[remote_loads_df['cache_result'] == result]\n",
    "    # print(subset)\n",
    "    plt.scatter(subset['time'], subset['phys'], label=result, color=color, s=6, alpha=0.5)\n",
    "    # break\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# if width is None :\n",
    "#     width = 28\n",
    "# if height is None :\n",
    "#     height = 12\n",
    "plt.gcf().set_size_inches(120, 60)\n",
    "plt.grid(axis=\"y\", which=\"both\")\n",
    "plt.grid(axis=\"x\", which=\"major\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Phys')\n",
    "plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "plt.legend(title='Cache Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing cpu loads and remote l3 misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_df = build_events_df(in_working_dir(\"perf.log\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of l3 remote cache for each CPU. Possibly can be multiplied by the period value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_count_per_cpu(df: pd.DataFrame, event: str) :\n",
    "    filtered_df = df.loc[df['event'] == event]\n",
    "    # cpu_sorted = [i for i in range(64) if i % 2 == 0] + [i for i in range(64) if i % 2 == 1]\n",
    "    cpu_remote_l3_loads = [filtered_df.loc[filtered_df['cpuid'] == cpuid]['period'].sum() for cpuid in range(64)] \n",
    "\n",
    "    bars = plt.bar(range(len(cpu_remote_l3_loads)), cpu_remote_l3_loads, width=0.8)\n",
    "    for item in bars[::2]:\n",
    "        item.set_color('tab:orange')\n",
    "    plt.ylim(0, max(cpu_remote_l3_loads) * 1.1)\n",
    "    plt.xticks([i for i in range(65) if i % 2 == 0])\n",
    "    plt.minorticks_on()\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    plt.tick_params(axis='x', which='minor', bottom=False)\n",
    "    plt.title(f\"Number {event} events per core\\nTotal \" + '{:.4e}'.format(sum(cpu_remote_l3_loads)) + \"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events_df['event'].unique())\n",
    "for event in events_df['event'].unique() :\n",
    "    plot_event_count_per_cpu(events_df, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to print the addresses of node 0 and node 1 independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phys_addresses(df: pd.DataFrame, cluster_eps: int, cluster_min_size: int) :\n",
    "    phys_clusters = get_cluster_bounds(df['phys'], cluster_eps, cluster_min_size)\n",
    "    \n",
    "    for node in [0, 1] :\n",
    "        node_df = df.loc[df['memory_node'] == node]\n",
    "        min_phys_addr = node_df['phys'].min()\n",
    "        max_phys_addr = node_df['phys'].max()\n",
    "        l3_remote_df = filter_l3_miss_retired_remote(node_df)\n",
    "        \n",
    "        plt.plot(node_df.index, node_df['phys'].sort_values())\n",
    "        for cluster in phys_clusters :\n",
    "            if min_phys_addr <= cluster[0] <= max_phys_addr :\n",
    "                plt.axhline(y=cluster[0], color='g', linestyle='--', linewidth=0.2)\n",
    "            if min_phys_addr <= cluster[1] <= max_phys_addr :\n",
    "                plt.axhline(y=cluster[1], color='g', linestyle='--', linewidth=0.2)\n",
    "\n",
    "        plt.scatter(l3_remote_df.index, l3_remote_df['phys'].sort_values(), s=3, color='r', alpha=0.1)    \n",
    "\n",
    "        plt.axhline(y=NODE_1_PHYS_ADDR_START, color='r', linestyle='--', linewidth=0.8)\n",
    "        plt.gcf().set_size_inches(40, 20)\n",
    "        plt.title(\"Physical addresses ordered\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phys_addresses(events_df, 3e8, 100)\n",
    "# plot_phys_addresses(events_df, 1, 3e8, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(events_df))\n",
    "phys_clusters = get_cluster_bounds(events_df['phys'].to_numpy(), 3e8, 4000)\n",
    "print(phys_clusters)\n",
    "\n",
    "events_df_sorted_phys = events_df.sort_values('phys', ignore_index=True)\n",
    "\n",
    "plt.plot(events_df_sorted_phys.index, events_df_sorted_phys['phys'])\n",
    "for cluster in phys_clusters :\n",
    "    plt.axhline(y=cluster[0], color='g', linestyle='--', linewidth=0.2)\n",
    "    plt.axhline(y=cluster[1], color='g', linestyle='--', linewidth=0.2)\n",
    "    \n",
    "for event in events_df_sorted_phys['event'].unique() :\n",
    "    if \"l3_miss_retired.remote_dram\" not in event:\n",
    "        continue\n",
    "    # plot_event_count_per_cpu(events_df_sorted_phys, event)\n",
    "    filtered_df = events_df_sorted_phys.loc[events_df_sorted_phys['event'] == event]\n",
    "    plt.scatter(filtered_df.index, filtered_df['phys'], label=event, c='r', s=2, alpha=0.8, zorder=10)\n",
    "\n",
    "# plt.scatter(l3_remote_df.index, l3_remote_df['phys'], s=2, color='r', alpha=0.1)\n",
    "\n",
    "plt.axhline(y=NODE_1_PHYS_ADDR_START, color='r', linestyle='--', linewidth=0.8)\n",
    "plt.gcf().set_size_inches(20, 20)\n",
    "plt.title(\"Physical addresses ordered\")\n",
    "plt.legend(title='Cache Result', loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# events_df_sorted_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_that_contain_event(df: pd.DataFrame, clusters: list, event: str):\n",
    "    # clusters could be converted to named tuple\n",
    "    res = []\n",
    "    for cluster in clusters:\n",
    "        bounded_subset = filter_in_bounds(df, 'phys', cluster)\n",
    "        event_subset = bounded_subset.loc[bounded_subset['event'] == event]\n",
    "        if len(event_subset) > 0 :\n",
    "            # display(event_subset)\n",
    "            res.append((cluster[0], cluster[1], len(event_subset), list(event_subset['cpuid'].unique())))\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_for_event = get_clusters_that_contain_event(events_df, phys_clusters, \"mem_load_l3_miss_retired.remote_dram:P\")\n",
    "clusters_for_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cpu/mem-loads/P' 'cpu/mem-stores/P'\n",
    "#  'mem_load_l3_miss_retired.local_dram:P'\n",
    "#  'mem_load_l3_miss_retired.remote_dram:P'\n",
    "#  'mem_load_l3_miss_retired.remote_hitm:P'\n",
    "#  'mem_load_l3_miss_retired.remote_fwd:P']\n",
    "\n",
    "def plot_phys(df: pd.DataFrame, phys_bounds: tuple, cpuid: int) :\n",
    "    cpu_df = df.loc[df['cpuid'] == cpuid]\n",
    "    bounded_df = filter_in_bounds(cpu_df, 'phys', phys_bounds)\n",
    "    \n",
    "    loads_df = bounded_df.loc[bounded_df['event'] == \"cpu/mem-loads/P\"]\n",
    "    stores_df = bounded_df.loc[bounded_df['event'] == \"cpu/mem-stores/P\"]\n",
    "    l3_miss_remote_df = bounded_df.loc[bounded_df['event'] == \"mem_load_l3_miss_retired.remote_dram:P\"]\n",
    "    \n",
    "    # loads_df = df.loc[df['event'] == \"cpu/mem-loads/p\"]\n",
    "    # l3_df = df.loc[df['event'] == \"mem_load_l3_miss_retired.remote_dram\"]\n",
    "    plt.scatter(loads_df['time'], loads_df['phys'], label=\"Memory loads\", s=6, alpha=0.5)\n",
    "    plt.scatter(stores_df['time'], stores_df['phys'], label=\"Memory stores\", s=6, alpha=0.5)\n",
    "    plt.scatter(l3_miss_remote_df['time'], l3_miss_remote_df['phys'], label=\"l3 miss remote dram\", color='r', s=6, alpha=0.5)\n",
    "        # break\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    # if width is None :\n",
    "    #     width = 28\n",
    "    # if height is None :\n",
    "    #     height = 12\n",
    "    # plt.gcf().set_size_inches(120, 60)\n",
    "    plt.gcf().set_size_inches(20, 30)\n",
    "    plt.grid(axis=\"y\", which=\"both\")\n",
    "    plt.grid(axis=\"x\", which=\"major\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phys')\n",
    "    plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "    plt.legend(title='Cache Result')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phys(events_df, clusters_for_event[5], 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_events_df = filter_in_bounds(events_df, \"phys\", phys_clusters[0])\n",
    "cpu_bounded_events_df = bounded_events_df.loc[bounded_events_df['cpuid'] == 1]\n",
    "\n",
    "cpu_events_df = events_df.loc[events_df['cpuid'] == 1]\n",
    "l3_df = filter_l3_miss_retired_remote(events_df.loc[events_df['cpuid'] == 1])\n",
    "# l3_df.loc[l3_df[\"cpu_node\"] == l3_df[\"memory_node\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_events_df['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_l3_miss_remote(df: pd.DataFrame):\n",
    "    loads_df = df.loc[df['event'] == \"cpu/mem-loads/p\"]\n",
    "    l3_df = df.loc[df['event'] == \"mem_load_l3_miss_retired.remote_dram\"]\n",
    "    plt.scatter(loads_df['time'], loads_df['phys'], label=\"loads df\", s=6, alpha=0.5)\n",
    "    plt.scatter(l3_df['time'], l3_df['phys'], label=\"l3 df\", color='r', s=6, alpha=0.5)\n",
    "        # break\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    # if width is None :\n",
    "    #     width = 28\n",
    "    # if height is None :\n",
    "    #     height = 12\n",
    "    # plt.gcf().set_size_inches(120, 60)\n",
    "    plt.gcf().set_size_inches(20, 30)\n",
    "    plt.grid(axis=\"y\", which=\"both\")\n",
    "    plt.grid(axis=\"x\", which=\"major\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phys')\n",
    "    plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "    plt.legend(title='Cache Result')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_events_df = events_df.loc[events_df['cpuid'] == 25]\n",
    "cpu_phys_bounds = get_cluster_bounds(filter_l3_miss_retired_remote(cpu_events_df)['phys'], 5e8, 500)\n",
    "print(cpu_phys_bounds)\n",
    "filtered_cpu_events_df = filter_in_bounds(cpu_events_df, 'phys', cpu_phys_bounds[0])\n",
    "plot_with_l3_miss_remote(filtered_cpu_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cpu_events_df['cpuid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

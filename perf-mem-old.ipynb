{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "import math as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_CMD_CACHE_FILENAME = \"trace_cmd_runtimes\"\n",
    "RESULTS_DIR_PATH = \"../results\"\n",
    "ABSOLUTE_RESULTS_DIR_PATH = \"/home/cgachod/analysis/results\"\n",
    "\n",
    "WORKING_DIR = \"/root/tests\"\n",
    "PERF_SCRIPT_RESULTS_FILEPATH = f\"{WORKING_DIR}/.perf_mem_results.log\"\n",
    "\n",
    "NODE_1_PHYS_ADDR_START = 0x1840000000\n",
    "\n",
    "DAHU_NODE_0_CPUID = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62]\n",
    "DAHU_NODE_1_CPUID = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63]\n",
    "dahu_cpu_nodes = [1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)]\n",
    "\n",
    "def in_working_dir(path: str) :\n",
    "    return os.path.join(WORKING_DIR, path)\n",
    "\n",
    "def get_result_dir_path(result_dir_name):\n",
    "    return os.path.join(RESULTS_DIR_PATH, result_dir_name)\n",
    "\n",
    "def get_absolute_result_dir_path(result_dir_name) :\n",
    "    return os.path.join(ABSOLUTE_RESULTS_DIR_PATH, result_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_enabled_color = \"tab:orange\"\n",
    "nb_disabled_color = \"tab:blue\"\n",
    "\n",
    "figure_height = 10 # 10\n",
    "min_figure_width = 40\n",
    "figure_width_coeff = 0.3\n",
    "\n",
    "line_width = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to represent where each core accesses memory\n",
    "- Sort cores by node\n",
    "- For each core, have a timeline or maybe just a percentage of time where it accesses which node\n",
    "- Maybe we can have 2 histograms, one up which is local node and 1 down which is remote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So for that we only need cpu and memory address and time\n",
    "# We assume we have the perf data file\n",
    "\n",
    "# 1: pid, 2: tid, 3: cpuid, 4: time, 5: period, 6: event , 7: virtual address\n",
    "# basic_info_regex_str = r\"^ *[\\w\\/\\-\\.\\: ]+ +(\\d+) \\[(\\d+)\\] (\\d+\\.\\d+): +([\\w\\/\\-\\.]+).*?: +([0-9a-f]+)\"\n",
    "basic_info_regex_str = r\"^ *[\\w\\/\\-\\.\\:]+ +(\\d+)\\/(\\d+) +\\[(\\d+)\\] +(\\d+\\.\\d+): +(\\d+) +([\\w\\/\\-\\.]+).*?: +([0-9a-f]+)\"\n",
    "\n",
    "# 8: memory access type, 9: TLB access type\n",
    "data_src_regex_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+\"\n",
    "phys_addr_regex_str = r\"([0-9a-f]+)\"\n",
    "# data_src_regex = re.compile()\n",
    "\n",
    "line_regex = re.compile(basic_info_regex_str + r\"\\s+\" + data_src_regex_str + phys_addr_regex_str)\n",
    "\n",
    "def generate_perf_mem_log(file_path: str) -> dict :\n",
    "    command_str = f\"perf script -i {file_path} -c cg.C.x -F 'comm,pid,tid,cpu,time,period,event,addr,data_src,phys_addr' > {PERF_SCRIPT_RESULTS_FILEPATH}\"\n",
    "    print(command_str)\n",
    "    result = subprocess.run(\n",
    "        command_str,\n",
    "        shell=True,\n",
    "        stdout = subprocess.PIPE,\n",
    "        universal_newlines = True\n",
    "    )\n",
    "    \n",
    "# perf script -L -F 'comm,pid,tid,cpu,time,addr,event,ip,phys_addr,data_src,period' --reltime --time 1.0,2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resgex = line_regex.match(\"          cg.C.x 10674/10674 [000]  2789.844649:      26661         cpu/mem-stores/P: ffffe9b88d9bfcd0      1e05080144 |OP STORE|LVL L1 or N/A hit|SNP N/A|TLB N/A|LCK N/A|BLK  N/A                                0\")\n",
    "print(resgex.group(6)[4:14] == \"mem-stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_perf_mem_log(in_working_dir(\"perf-mem-sequential.data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testval = ([], [])\n",
    "testval[0].append(9)\n",
    "\n",
    "print(testval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_memory_accesses_dfs(filepath: str) -> pd.DataFrame :\n",
    "    # loads, stores\n",
    "    timestamp = ([], []) \n",
    "    cpuid = ([], [])\n",
    "    virtual_addr = ([], [])\n",
    "    physical_addr = ([], [])\n",
    "    cache_result = ([], [])\n",
    "    period = ([], [])\n",
    "\n",
    "    # filename = \"assets/perf_mem_sample.log\"\n",
    "    # filename = \"assets/.perf_mem_results.log\"\n",
    "    with open(filepath) as f :\n",
    "        for line in f :\n",
    "            matched = line_regex.match(line)\n",
    "            if matched :\n",
    "                load_store_idx = int(matched[6][4:14] == \"mem-stores\")\n",
    "                timestamp[load_store_idx].append(float(matched[4]))\n",
    "                cpuid[load_store_idx].append(int(matched[3]))\n",
    "                cache_result[load_store_idx].append(matched[8])\n",
    "                virtual_addr[load_store_idx].append(int(matched[7], base=16))\n",
    "                physical_addr[load_store_idx].append(int(matched[10], base=16))\n",
    "                period[load_store_idx].append(int(matched[5]))\n",
    "                pass\n",
    "            else :\n",
    "                print(\"Not matched line : \", line)\n",
    "                \n",
    "    dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "    \n",
    "    memory_dfs = []\n",
    "    for i in range(2) :\n",
    "        df = pd.DataFrame({\n",
    "            \"time\": timestamp[i], \n",
    "            \"cpuid\": cpuid[i],  \n",
    "            \"virt\": virtual_addr[i], \n",
    "            \"phys\": physical_addr[i], \n",
    "            \"cache_result\": cache_result[i], \n",
    "            \"period\": period[i]\n",
    "        })\n",
    "        df['time_offset'] = df['time'].diff()\n",
    "        df['cpu_node'] = df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "        df['memory_node'] = (df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "        df['time'] = df['time'] - df['time'].min()\n",
    "        memory_dfs.append(df.loc[df['phys'] != 0])\n",
    "        \n",
    "                \n",
    "    # # memory_dfs = [\n",
    "    # #     ),\n",
    "    # #     pd.DataFrame(\n",
    "    # #     {\"time\": timestamp, \"cpuid\": cpuid,  \"virt\": virtual_addr, \"phys\": physical_addr , \"cache_result\": cache_result, \"period\": period})\n",
    "    # # ]\n",
    "\n",
    "    # # print(NODE_1_PHYS_ADDR_START)\n",
    "    # memory_df = \n",
    "\n",
    "    # memory_loads_df = 0\n",
    "    # memory_stores_df =     \n",
    "\n",
    "    # memory_df['cpu_node'] = memory_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "    # memory_df['memory_node'] = (memory_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "    # memory_df['time'] = memory_df['time'] - memory_df['time'].min()\n",
    "    # memory_df['time_offset'] = memory_df['time'].diff()\n",
    "    # return memory_df.loc[memory_df['phys'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_memory_accesses_dfs(filepath: str) -> Tuple[pd.DataFrame, pd.DataFrame] :\n",
    "    # loads, stores\n",
    "    is_store = []\n",
    "    timestamp = []\n",
    "    cpuid = []\n",
    "    virtual_addr = []\n",
    "    physical_addr = []\n",
    "    cache_result = []\n",
    "    period = []\n",
    "\n",
    "    # filename = \"assets/perf_mem_sample.log\"\n",
    "    # filename = \"assets/.perf_mem_results.log\"\n",
    "    with open(filepath) as f :\n",
    "        for line in f :\n",
    "            matched = line_regex.match(line)\n",
    "            if matched :\n",
    "                is_store.append(int(matched[6][4:14] == \"mem-stores\"))\n",
    "                timestamp.append(float(matched[4]))\n",
    "                cpuid.append(int(matched[3]))\n",
    "                cache_result.append(matched[8])\n",
    "                virtual_addr.append(int(matched[7], base=16))\n",
    "                physical_addr.append(int(matched[10], base=16))\n",
    "                period.append(int(matched[5]))\n",
    "                pass\n",
    "            else :\n",
    "                print(\"Not matched line : \", line)\n",
    "                \n",
    "    dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "    \n",
    "    accesses_df = pd.DataFrame({\n",
    "        \"time\": timestamp, \n",
    "        \"cpuid\": cpuid,  \n",
    "        \"virt\": virtual_addr, \n",
    "        \"phys\": physical_addr, \n",
    "        \"cache_result\": cache_result, \n",
    "        \"period\": period,\n",
    "        \"is_store\": is_store\n",
    "    })\n",
    "    accesses_df['time'] = accesses_df['time'] - accesses_df['time'].min()\n",
    "    # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "    accesses_df['cpu_node'] = accesses_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "    accesses_df['memory_node'] = (accesses_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "    accesses_df = accesses_df.loc[accesses_df['phys'] != 0]\n",
    "    \n",
    "    loads_df = accesses_df.loc[accesses_df[\"is_store\"] == 0]\n",
    "    loads_df['time_offset'] = loads_df['time'].diff()\n",
    "    \n",
    "    stores_df = accesses_df.loc[accesses_df[\"is_store\"] == 1]\n",
    "    stores_df['time_offset'] = stores_df['time'].diff()\n",
    "    \n",
    "    return loads_df.iloc[1:], stores_df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 664 894 lines\n",
    "loads_df, stores_df = build_memory_accesses_dfs(in_working_dir(\"perf_mem_sample.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores_df.loc[stores_df['cache_result'] == \"LVL L1 or N/A hit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.cm.Spectral_r([0,1])\n",
    "\n",
    "# for colorname in plt.colormaps() :\n",
    "#     display(plt.cm.get_cmap(colorname))\n",
    "# colormaps[\"turbo\"]([100, 128, 196, 70])\n",
    "# plt.cm.get_cmap(\"OrRd\")\n",
    "\n",
    "# colormaps[\"OrRd\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_results_stats(input_df: pd.DataFrame) :\n",
    "    period_mean = []\n",
    "    time_offset_mean = []\n",
    "    cache_results_counts = input_df['cache_result'].value_counts()\n",
    "    for key, value in cache_results_counts.items() :\n",
    "        period_mean.append(input_df.loc[input_df['cache_result'] == key]['period'].mean())\n",
    "        time_offset_mean.append(input_df.loc[input_df['cache_result'] == key]['time_offset'].mean())\n",
    "    result_df = cache_results_counts.to_frame()\n",
    "    result_df['period_mean'] = period_mean\n",
    "    result_df['time_offset_mean'] = time_offset_mean\n",
    "    return result_df.sort_values('period_mean', ascending=False)\n",
    "\n",
    "\n",
    "def plot_cache_results(input_df: pd.DataFrame) :\n",
    "    period_per_cr = {}\n",
    "    time_offset_per_cr = {}\n",
    "    cache_results_counts = input_df['cache_result'].value_counts()\n",
    "    for key, value in cache_results_counts.items() :\n",
    "        period_per_cr[key] = input_df.loc[input_df['cache_result'] == key]['period']\n",
    "        time_offset_per_cr[key] = input_df.loc[input_df['cache_result'] == key]['time_offset']\n",
    "        \n",
    "    # Period values\n",
    "    plt.boxplot(period_per_cr.values(), labels=period_per_cr.keys())\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Time offsets values\n",
    "    plt.boxplot(time_offset_per_cr.values(), labels=time_offset_per_cr.keys(), showfliers=False, showmeans=True)\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    # plt.gcf().set_size_inches(12, 50)\n",
    "    plt.show()\n",
    "    \n",
    "    ratio = {key: period_per_cr[key] / time_offset_per_cr[key] for key in period_per_cr.keys()}\n",
    "    plt.boxplot(ratio.values(), labels=period_per_cr.keys(), showfliers=False, showmeans=True)\n",
    "    plt.xticks(rotation=20, ha='right')\n",
    "    # plt.gcf().set_size_inches(12, 50)\n",
    "    plt.show()\n",
    "    \n",
    "plot_cache_results(loads_df)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loads cache results stats\")\n",
    "print(get_cache_results_stats(loads_df))\n",
    "\n",
    "print(\"\\n\\nStores cache results stats\")\n",
    "print(get_cache_results_stats(stores_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_accesses_for_cpu(accesses_df: pd.DataFrame, cpuid = -1, min_phys = None max_phys = None) :\n",
    "    cache_results_df = get_cache_results_stats(accesses_df)\n",
    "    color_map = colormaps[\"turbo\"]\n",
    "    cache_results_colors = {cache_res_val: color_map(25 * i) for i, cache_res_val in enumerate(period_df.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# remote_memory_df['cache_result'].value_counts().items() # I want to have value counts and average period for each\n",
    "period_df = get_cache_results_stats(loads_df)\n",
    "period_df = period_df.sort_values('period_mean', ascending=False)\n",
    "\n",
    "print(period_df)\n",
    "\n",
    "color_map = colormaps[\"turbo\"]\n",
    "cache_results_colors = {cache_res_val: color_map(25 * i) for i, cache_res_val in enumerate(period_df.index)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_1D(values: np.array, eps: int, min_size: int) -> Tuple[list, list] :\n",
    "    sorted_values = np.sort(values)\n",
    "    prev_val = np.concatenate(([sorted_values[0]],sorted_values[:-1])) # n - 1\n",
    "    virtual_addresses_minus_diff = np.absolute(sorted_values - prev_val) # addr[i] - addr[i - 1]\n",
    "    \n",
    "    indices = np.where(virtual_addresses_minus_diff > eps)[0]\n",
    "    # print(range)\n",
    "    \n",
    "    clusters_bounds = [] # (min, max excluded)\n",
    "    clusters_lengths = []\n",
    "    if indices[0] >= min_size :\n",
    "        clusters_bounds.append( (0, sorted_values[indices[0]]) )\n",
    "        clusters_lengths.append(indices[0])\n",
    "    for i in range(len(indices) - 1) :\n",
    "        cluster_len = indices[i + 1] - indices[i]\n",
    "        if cluster_len < min_size :\n",
    "            continue\n",
    "        clusters_bounds.append(( sorted_values[indices[i]], sorted_values[indices[i + 1]] ))\n",
    "        clusters_lengths.append(cluster_len)\n",
    "    \n",
    "    return clusters_bounds, clusters_lengths\n",
    "\n",
    "\n",
    "# def get_virtual_addrs_clusters(data: dict) -> Tuple[list, list] :\n",
    "#     return get_clusters_1D(np.concatenate([np.array(cpu['virtual_addrs']) for cpu in data]), 1e7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clusters_1D(loads_df['phys'], 1e8, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = get_clusters_1D(loads_df['phys'], 1e8, 100000)[0][3]\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(remote_memory_df['phys'].index, remote_memory_df['phys'].sort_values())\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan :\n",
    "- Have a graph of the exact phys remote addresses, maybe we can see smth there\n",
    "- Otherwise scatter all the addresses but y axis only remote vs local node, and color according to type of cache hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_addr_subset_df = memory_df.loc[(memory_df['phys'] > bounds[0]) & (memory_df['phys'] < bounds[1])]\n",
    "phys_addr_subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuid = 1\n",
    "# subset_df = memory_df.loc[(memory_df['cache_result'] in [\"LVL L1 or L1 hit\", LVL Remote RAM hit])]\n",
    "subset_df = phys_addr_subset_df.loc[memory_df['cpuid'] == cpuid]\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(subset_df[\"time\"], subset_df[\"phys\"], s=6, alpha=0.5, c=colors[n])\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_loads_df = loads_df.loc[loads_df['cpu_node'] != loads_df['memory_node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_loads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_memory_df\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# colors = \n",
    "# colors = {result: plt.cm.tab20(i) for i, result in enumerate(remote_memory_df['cache_result'].unique())}\n",
    "cache_results_colors\n",
    "for result, color in cache_results_colors.items():\n",
    "    # print(\"Cache result :\", result, \"color : \")\n",
    "    subset = remote_loads_df.loc[remote_loads_df['cache_result'] == result]\n",
    "    # print(subset)\n",
    "    plt.scatter(subset['time'], subset['phys'], label=result, color=color, s=6, alpha=0.5)\n",
    "    # break\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# if width is None :\n",
    "#     width = 28\n",
    "# if height is None :\n",
    "#     height = 12\n",
    "plt.gcf().set_size_inches(120, 60)\n",
    "plt.grid(axis=\"y\", which=\"both\")\n",
    "plt.grid(axis=\"x\", which=\"major\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Phys')\n",
    "plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "plt.legend(title='Cache Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

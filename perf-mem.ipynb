{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.cluster import DBSCAN\n",
    "import math as ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_CMD_CACHE_FILENAME = \"trace_cmd_runtimes\"\n",
    "RESULTS_DIR_PATH = \"../results\"\n",
    "ABSOLUTE_RESULTS_DIR_PATH = \"/home/cgachod/analysis/results\"\n",
    "\n",
    "WORKING_DIR = \"/root/tests\"\n",
    "PERF_SCRIPT_RESULTS_FILEPATH = f\"{WORKING_DIR}/.perf_mem_results.log\"\n",
    "\n",
    "NODE_1_PHYS_ADDR_START = 0x1840000000\n",
    "\n",
    "DAHU_NODE_0_CPUID = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62]\n",
    "DAHU_NODE_1_CPUID = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63]\n",
    "dahu_cpu_nodes = [1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)]\n",
    "\n",
    "def in_working_dir(path: str) :\n",
    "    return os.path.join(WORKING_DIR, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerfResultsReader:\n",
    "    def __init__(self, log_file_path = in_working_dir(\".perf.log\")) -> None:\n",
    "        self.log_file_path = log_file_path\n",
    "        \n",
    "    def __get_initial_timestamp(self, data_file_path: str) :\n",
    "        res = subprocess.run(\n",
    "            f\"perf script -i {data_file_path} -F time | head -1\",\n",
    "            shell=True,\n",
    "            stdout = subprocess.PIPE,\n",
    "            universal_newlines = True\n",
    "        )\n",
    "        return float(res.stdout.strip(':\\n '))\n",
    "        \n",
    "    # For example time_option=\"10%-20%\"\n",
    "    def __extract_perf_data_file_with_latency(self, data_file_path: str, executable: Optional[str], time_option: Optional[str]) -> float:\n",
    "        initial_timestamp = self.__get_initial_timestamp(data_file_path)\n",
    "        print(f\"Retrieved initial timestamp : {initial_timestamp}\")\n",
    "        \n",
    "        executable_filter = f\"-c {executable}\" if executable is not None else \"\"\n",
    "        time_filter = f\"--time {time_option}\" if time_option is not None else \"\"\n",
    "        command_str = f\"perf script -i {data_file_path} -L {executable_filter} {time_filter} > {self.log_file_path}\"\n",
    "        print(f\"Executing {command_str}\")\n",
    "        result = subprocess.run(\n",
    "            command_str,\n",
    "            shell=True,\n",
    "            stdout = subprocess.PIPE,\n",
    "            universal_newlines = True\n",
    "        )\n",
    "        return initial_timestamp\n",
    "    \n",
    "    def __parse_events_with_latency(self, filepath: str, initial_timestamp: float = 0.0) -> pd.DataFrame :\n",
    "        # 1: pid, 2: cpuid, 3: timestamp, 4: period, 5: event, 6: virt_addr \n",
    "        basic_info_regex_lat_str = r\"^ *\\S+ +(\\d+) +(\\d+) +(\\d+\\.\\d+): +(\\d+) +(\\S+): +([0-9a-f]+)\"\n",
    "        \n",
    "        # 1: cache_result, 2: tlb_result, 3: latency, 4: phys_adress\n",
    "        data_src_regex_lat_str = r\"[0-9a-f]+ \\|OP (?:LOAD|STORE)\\|([^\\|]+)\\|[^\\|]+\\|(TLB [^\\|]+)\\|[^\\|]+\\|[a-zA-Z\\/\\- ]+(\\d+) +\\d+ +[0-9a-f]+.+ ([0-9a-f]+)\"\n",
    "        line_lat_regex = re.compile(basic_info_regex_lat_str + r\" +\" + data_src_regex_lat_str)\n",
    "        \n",
    "        # pid = []\n",
    "        cpuid = []\n",
    "        timestamp = []\n",
    "        period = []\n",
    "        event = []\n",
    "        virtual_addr = []\n",
    "        cache_result = []\n",
    "        latency = []\n",
    "        physical_addr = []\n",
    "\n",
    "        with open(filepath) as f :\n",
    "            for line in f :\n",
    "                matched = line_lat_regex.match(line)\n",
    "                if matched :\n",
    "                    # pid.append(int(matched[1]))\n",
    "                    cpuid.append(int(matched[2]))\n",
    "                    timestamp.append(float(matched[3]))\n",
    "                    period.append(int(matched[4]))\n",
    "                    event.append(matched[5])\n",
    "                    virtual_addr.append(int(matched[6], base=16))\n",
    "                    cache_result.append(matched[7])\n",
    "                    latency.append(int(matched[9]))\n",
    "                    physical_addr.append(int(matched[10], base=16))\n",
    "                    pass\n",
    "                else :\n",
    "                    print(\"Not matched line : \", line)\n",
    "                    \n",
    "        dahu_cpu_nodes_map = {cpuid: 1 if cpuid in DAHU_NODE_1_CPUID else 0 for cpuid in range(64)}\n",
    "        \n",
    "        events_df = pd.DataFrame({\n",
    "            \"cpuid\": cpuid,  \n",
    "            \"time\": timestamp, \n",
    "            \"period\": period,\n",
    "            \"event\": event,\n",
    "            \"virt\": virtual_addr,\n",
    "            \"phys\": physical_addr, \n",
    "            \"latency\": latency,\n",
    "            \"cache_result\": cache_result,\n",
    "        })\n",
    "        events_df['time'] = events_df['time'] - initial_timestamp\n",
    "        # accesses_df['time_offset'] = accesses_df['time'].diff()\n",
    "        events_df['cpu_node'] = events_df['cpuid'].map(dahu_cpu_nodes_map)\n",
    "        events_df['memory_node'] = (events_df['phys'] >= NODE_1_PHYS_ADDR_START).astype(int)\n",
    "        return events_df\n",
    "    \n",
    "    def read_perf_data_with_latency(self, data_file_path: str, executable: Optional[str] = None, time_option: Optional[str] = None) -> pd.DataFrame:\n",
    "        initial_timestamp = self.__extract_perf_data_file_with_latency(data_file_path, executable, time_option)\n",
    "        df = self.__parse_events_with_latency(self.log_file_path, initial_timestamp)\n",
    "        os.remove(self.log_file_path)\n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr = PerfResultsReader()\n",
    "# prr.extract_perf_data_file_with_latency(data_file_path=in_working_dir(\"perf.data\"), executable=\"cg.C.x\")\n",
    "# prr._PerfResultsReader__parse_events_with_latency(in_working_dir(\".perf.sample.log\"))\n",
    "events_df = prr.read_perf_data_with_latency(in_working_dir(\"perf-mem-sequential.data\"), \"cg.C.x\", time_option=\"10%-12%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_bounds(values: np.array, eps: int, min_group_size: int):\n",
    "    sorted_values = np.sort(values)\n",
    "    \n",
    "    clusters = []\n",
    "    curr_lower_idx = 0\n",
    "    curr_lower_val = sorted_values[0]\n",
    "    # current_group = [sorted_values[0]]\n",
    "\n",
    "    for i, val in enumerate(sorted_values):\n",
    "        group_size = i - curr_lower_idx - 1\n",
    "        if val - curr_lower_val > eps and group_size >= min_group_size :\n",
    "            clusters.append((curr_lower_val, sorted_values[i - 1], group_size))\n",
    "            curr_lower_idx = i\n",
    "            curr_lower_val = val\n",
    "            \n",
    "    return clusters\n",
    "\n",
    "def filter_in_bounds(df: pd.DataFrame, property: str, bounds: tuple) -> pd.DataFrame :\n",
    "    return df.loc[(df[property] > bounds[0]) & (df[property] < bounds[1])]\n",
    "\n",
    "\n",
    "# def get_virtual_addrs_clusters(data: dict) -> Tuple[list, list] :\n",
    "#     return get_clusters_1D(np.concatenate([np.array(cpu['virtual_addrs']) for cpu in data]), 1e7, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing cpu loads and remote l3 misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of l3 remote cache for each CPU. Possibly can be multiplied by the period value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_count_per_cpu(df: pd.DataFrame, event: str) :\n",
    "    filtered_df = df.loc[df['event'] == event]\n",
    "    # cpu_sorted = [i for i in range(64) if i % 2 == 0] + [i for i in range(64) if i % 2 == 1]\n",
    "    cpu_remote_l3_loads = [filtered_df.loc[filtered_df['cpuid'] == cpuid]['period'].sum() for cpuid in range(64)] \n",
    "\n",
    "    bars = plt.bar(range(len(cpu_remote_l3_loads)), cpu_remote_l3_loads, width=0.8)\n",
    "    for item in bars[::2]:\n",
    "        item.set_color('tab:orange')\n",
    "    plt.ylim(0, max(cpu_remote_l3_loads) * 1.1)\n",
    "    plt.xticks([i for i in range(65) if i % 2 == 0])\n",
    "    plt.minorticks_on()\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    plt.tick_params(axis='x', which='minor', bottom=False)\n",
    "    plt.title(f\"Number {event} events per core\\nTotal \" + '{:.4e}'.format(sum(cpu_remote_l3_loads)) + \"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events_df['event'].unique())\n",
    "for event in events_df['event'].unique() :\n",
    "    plot_event_count_per_cpu(events_df, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(events_df))\n",
    "phys_clusters = get_cluster_bounds(events_df['phys'].to_numpy(), 1e7, 4000)\n",
    "print(phys_clusters)\n",
    "\n",
    "events_df_sorted_phys = events_df.sort_values('phys', ignore_index=True)\n",
    "\n",
    "plt.plot(events_df_sorted_phys.index, events_df_sorted_phys['phys'])\n",
    "for cluster in phys_clusters :\n",
    "    plt.axhline(y=cluster[0], color='g', linestyle='--', linewidth=0.2)\n",
    "    plt.axhline(y=cluster[1], color='g', linestyle='--', linewidth=0.2)\n",
    "    \n",
    "for event in events_df_sorted_phys['event'].unique() :\n",
    "    if \"l3_miss_retired.remote_dram\" not in event:\n",
    "        continue\n",
    "    # plot_event_count_per_cpu(events_df_sorted_phys, event)\n",
    "    filtered_df = events_df_sorted_phys.loc[events_df_sorted_phys['event'] == event]\n",
    "    plt.scatter(filtered_df.index, filtered_df['phys'], label=event, c='r', s=2, alpha=0.8, zorder=10)\n",
    "\n",
    "# plt.scatter(l3_remote_df.index, l3_remote_df['phys'], s=2, color='r', alpha=0.1)\n",
    "\n",
    "plt.axhline(y=NODE_1_PHYS_ADDR_START, color='r', linestyle='--', linewidth=0.8)\n",
    "plt.gcf().set_size_inches(20, 20)\n",
    "plt.title(\"Physical addresses ordered\")\n",
    "plt.legend(title='Cache Result', loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# events_df_sorted_phys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EventCluster = namedtuple('EventCluster', ['min_val', 'max_val', 'total_count', 'count_per_cpu'])\n",
    "\n",
    "def get_clusters_that_contain_event(df: pd.DataFrame, clusters: list, event: str):\n",
    "    # clusters could be converted to named tuple\n",
    "    res = []\n",
    "    for cluster in clusters:\n",
    "        bounded_subset = filter_in_bounds(df, 'phys', cluster)\n",
    "        event_subset = bounded_subset.loc[bounded_subset['event'] == event]\n",
    "        if len(event_subset) > 0 :\n",
    "            cpuids = event_subset['cpuid'].unique()\n",
    "            count_per_cpu: List[Tuple[int, int]] = []\n",
    "            for cid in cpuids:\n",
    "                number = (event_subset['cpuid'] == cid).sum()\n",
    "                count_per_cpu.append((number, cid))\n",
    "            # display(event_subset)\n",
    "            res.append((cluster[0], cluster[1], len(event_subset), sorted(count_per_cpu, reverse=True)))\n",
    "    return sorted(res, reverse=True, key=lambda x: x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_for_event = get_clusters_that_contain_event(events_df, phys_clusters, \"mem_load_l3_miss_retired.remote_dram:P\")\n",
    "for i, cluster in enumerate(clusters_for_event) :\n",
    "    print(f\"Cluster {i} :\", cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cpu/mem-loads/P' 'cpu/mem-stores/P'\n",
    "#  'mem_load_l3_miss_retired.local_dram:P'\n",
    "#  'mem_load_l3_miss_retired.remote_dram:P'\n",
    "#  'mem_load_l3_miss_retired.remote_hitm:P'\n",
    "#  'mem_load_l3_miss_retired.remote_fwd:P']\n",
    "\n",
    "def plot_phys(df: pd.DataFrame, phys_bounds: tuple, cpuid = [], exclude_cpuid = [], min_t = None, max_t = None) :\n",
    "    cpu_df = df.loc[df['cpuid'].isin(cpuid)] if cpuid else df\n",
    "    cpu_df = df.loc[~df['cpuid'].isin(exclude_cpuid)] if exclude_cpuid else cpu_df\n",
    "    bounded_df = filter_in_bounds(cpu_df, 'phys', phys_bounds)\n",
    "    \n",
    "    if min_t is not None and max_t is not None :\n",
    "        bounded_df = filter_in_bounds(bounded_df, 'time', (min_t, max_t))\n",
    "        \n",
    "    loads_df = bounded_df.loc[bounded_df['event'] == \"cpu/mem-loads/P\"]\n",
    "    stores_df = bounded_df.loc[bounded_df['event'] == \"cpu/mem-stores/P\"]\n",
    "    l3_miss_remote_df = bounded_df.loc[bounded_df['event'] == \"mem_load_l3_miss_retired.remote_dram:P\"]\n",
    "    \n",
    "    plt.scatter(loads_df['time'], loads_df['phys'], label=\"Memory loads\", color='tab:green', s=3, alpha=0.5, zorder=10)\n",
    "    plt.scatter(stores_df['time'], stores_df['phys'], label=\"Memory stores\", color='tab:green', s=3, alpha=0.5, zorder=10)\n",
    "    plt.scatter(l3_miss_remote_df['time'], l3_miss_remote_df['phys'], label=\"l3 miss remote dram\", color='r', s=3, alpha=0.5, zorder=10)\n",
    "    \n",
    "    cpuids = bounded_df['cpuid'].unique()\n",
    "    for cid in cpuids :\n",
    "        curr_df = bounded_df.loc[bounded_df['cpuid'] == cid]\n",
    "        plt.plot(curr_df['time'], curr_df['phys'], label=f\"CPU {cid}\", linestyle='--', linewidth=0.4, alpha=0.5)\n",
    "        \n",
    "        # break\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    # if width is None :\n",
    "    #     width = 28\n",
    "    # if height is None :\n",
    "    #     height = 12\n",
    "    # plt.gcf().set_size_inches(120, 60)\n",
    "    plt.gcf().set_size_inches(60, 30)\n",
    "    plt.grid(axis=\"y\", which=\"both\")\n",
    "    plt.grid(axis=\"x\", which=\"major\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phys')\n",
    "    plt.title('Scatter Plot of Time vs Phys with Cache Results Colored')\n",
    "    plt.legend(title='Cache Result')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_phys(events_df, clusters_for_event[14], exclude_cpuid=[62], min_t=1.40, max_t=1.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phys(events_df, clusters_for_event[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
